# CPU-only Dockerfile for LLM-Pipeline inference
FROM python:3.11-slim
WORKDIR /code

# Copy only requirements and leverage pip cache
COPY requirementsCPU.txt ./
RUN pip install --no-cache-dir -r requirementsCPU.txt


# Copy project code for inference
COPY inference/ ./inference
COPY model/ ./model
COPY models/ ../models
COPY config/ ../config

# Expose HTTP port for API
EXPOSE 80

# Launch FastAPI server
CMD ["uvicorn", "inference.inference:app", "--host", 'localhost', "--port", "80"]
