# confing/config.yaml
defaults:
  - override hydra/job_logging: disabled

# NOTE: the term "mixed_precision" might be unclear, it actually refers to fp16 and not bf16.

# model hyperparameters
model:
  model_name: "Maynx/REX_v0.1"

# training settings
training:
  run_name: "basic_lm_experiment"
  dataset_file_path: "your/path/to/the/jsonl/file"
  tokenizer_name: "REX_v0.1"
  max_length: 1024
  train_val_ratio: 0.95
  output_dir: "./out"
  num_epochs: 2 # Learning phase + checking epoch
  batch_size: 16 # Really depends on the device, e.g 16 would work on a L4.


# inference settings
inference:
  kv_cache: false # for now leave it false, because of some bugs. (Read model.py for more information)
  quantized: false # You can turn this on, it is supported. However, it may lead to worse results for now (waiting for RLHF later).
  mixed_precision: true # fp16 inference, is supported, but for bf16, you might need to go to the inference file and change it yourself, since I judge that very few people have bf16 devices.